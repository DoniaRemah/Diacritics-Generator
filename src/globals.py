training_sentences = []
test_sentences = []
# cleaned_train_sentences = []
tokenized_words = []
tokenized_chars = []
golden_outputs = []
word_vocabulary = set()
tokenized_sentences = []
unclean_sentences = []
clean_sentences=[]
# a list of tuples, each one contains a word and a list of tuples of char and diacritic
golden_outputs_list = []